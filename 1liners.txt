#removes end of line on fasta files
awk '!/^>/ { printf "%s", $0; n = "\n" } /^>/ { print n $0; n = "" } END { printf "%s", n }' PairedContig_297 > PairedContig_297.fa

#passes fast files to xargs then does stuff and renames out files
ls *.fa | xargs -I{} sh -c "head -n -101 '{}' > '{}.Nomt.fa'"

#removes sequence vivax15 and next lines until file end
sed '/>vivax15/,$d' foo.fa

#replace all spaces w/ a single tab thanks PLINK format
perl -pe 's/\s{2,}/\t/g' plink.genome

#change spaces to tabs
tr -s ' ' '\t' < students.txt
sed 's/ /\t/g' students.txt

#read from a text file pass to variable
cat FILE | while read FOO; do

#reattach a screen after forced logout
screen -d -r 7728.irssi

#deletes empty files
find /path/to/dir -empty -type f -delete

#substitute pattern 1st by pattern 2nd in Field6, print all to file
awk '{gsub("FIRST","SECOND",$6); print }' FILE.IN > FILE.OUT

#calculates the average of column 5
awk '{sum+=$5} END { print "Average = ",sum/NR}'

#counts the number of occurrences in column 5 greater than .2 and less than .8, prints count
awk '{ if ($5 >= 0.2 && $5 <= 0.8) count++ } END { print count }'

#remove last line
sed -i '$ d' foo.txt

#remove first line
sed 1d file > newfile

#turn 5 spaces into a tab
unexpand -a -t5

#awk command that cut down to 70bp   
awk '!/^@/{$10=substr($10,0,70);$11=substr($11,0,70);print}' Wb_reads_mtDNA_1019.sam > Wb_mtDNA_EndClip.sam

#takes first 1000 reads of bam file converts to sam file and stores in test.bam
samtools view -h 4_STS_WB1_pt22_PNGmtDNA.bam| head -n 1000 | samtools view -S -u - > test.bam 

#gets insert size from bam
samtools view -X Wb-702_mtGenome.bam | awk '$2 ~ /P/{print $9}' > insert_size.txt 

#only unique alignments from samtools
sed '/XS:/d' your_alignment_file.sam > your_alignment_file_1alignmentonly.sam

#returns max and min from a column	
awk 'NR>1 && max<$4{max=$4} END{print max}' WbPNGL3_POP_freebayes.Tajima.D
awk 'NR>1 && min>$4{min=$4} END{print min}' WbPNGL3_POP_freebayes.Tajima.D

#removes PairedContig_ and replaces w/ blank
sed -i.bak s/PairedContig_/""/g WbPNGL3_POP-filt.snps.freebayes.out.vcf 

#paste by columns
paste file1 file2 | column -s $'\t' -t

#replaces things in FILE1 with thing in FILE2 by matching; in this case I added contig lengths to freebayes vcf.
awk -F, 'NR==FNR{a[$1]=$0;next;}a[$1]{$0=a[$1]}1' file2 file1 > out
	#1. first buffer the file2 data in the form of key, value pair (associative array). and I have used key as col1 and value as entire line.
	#2. then check whether array key is exist for column 1 in file1 (joining columns) if yes, then replace file1 current value (i.e $0) with current value in array, if not skip it.
	#3. finally print the data.
	#e.g. awk -F, 'NR==FNR{a[$1]=$0;next;}a[$1]{$0=a[$1]}1' contig_lengths.txt WbPNGL3_POP-filt.snps.freebayes.out.vcf > WbPNGL3_POP_LEN-filt.snps.freebayes.out.vcf

#replaces non-ACGT codes with "N" in the REF column; T/T=T T/C=Y
awk '{gsub(/W|K|Y|R|S|M/,"N",$4); OFS = "\t"; print}' Input.vcf > Input_Ns.vcf

#Calculates the difference between 2 columns, here used to get the length of LowComplexityRegions output by mdust
awk 'NR == 1 { $5 = "diff." } NR >= 3 { $5 = $2 - $4 } 1' <input.txt

#checks that the reference allele field is not a complex variant as output by freebayes, as this causes errors w/ vcftools
awk -F"\t" 'length($4) > 1'

#replaces ambiguous bases in reference w/ N; fix_ambig
awk -F$'\t' -v OFS='\t' '{if ($0 !~ /^#/) gsub(/[KMRYSWBVHDX]/, "N", $4) } {print}'

#removes lines with an N as reference base; aka masked positions
awk '$4 !~ /N+/ {print}' INFILE > OUTFILE

#removes lines where the alt allele is longer than 2 or [ATCGN],[ACTGN] more than 1 alt allele
awk -F"\t" '$5 !~ /[ATCGN][ATCGN]+/ {print}' test.noN.length.vcf